# ğŸ“Š Task 05 â€“ Descriptive Statistics & LLM Evaluation

**Dataset**: [World Happiness Report 2019 (Kaggle)](https://www.kaggle.com/datasets/unsdsn/world-happiness)

---

## ğŸ§  Task Overview

This task focuses on analyzing the **World Happiness Report 2019** using statistical methods and evaluating the performance of a **Large Language Model (LLM)** in interpreting real-world data.

The task involves:

- Generating **descriptive, basic, advanced, and complex statistics** using Python and Pandas.
- Forming **natural language questions** about the dataset.
- Recording LLM-generated answers to those questions.
- Writing a script to compute the **actual, factual answers** from the dataset.
- Comparing and **evaluating** the LLM responses based on accuracy.

---

## ğŸ—‚ Repository Structure

```
â”œâ”€â”€ data/
â”‚ â””â”€â”€ 2019.csv # Dataset file (excluded via .gitignore)
â”œâ”€â”€ outputs/
â”‚ â”œâ”€â”€ basic_stats.txt # Summary statistics output
â”‚ â””â”€â”€ factual_answers.json # Script-generated factual answers
â”œâ”€â”€ .gitignore # To exclude data and output files from Git
â”œâ”€â”€ basic_stats.py # Script to generate descriptive and advanced statistics
â”œâ”€â”€ generate_factual_answers.py # Script to compute correct answers for all questions
â”œâ”€â”€ fill_correct_answers.py # Script to inject correct answers into llm_questions.md
â”œâ”€â”€ llm_questions.md # LLM prompts, responses, correct answers, and evaluations
â””â”€â”€ README.md # Project documentation

```
---

## âš™ï¸ How to Run

1. Place the dataset file as `data/2019.csv`  
   (Download from [here](https://www.kaggle.com/datasets/unsdsn/world-happiness))

2. Run the script to generate descriptive and advanced statistics:  
   python basic_stats.py

3. Generate factual answers from the dataset:  
   python generate_factual_answers.py

4. Inject factual answers into the markdown file:  
   python fill_correct_answers.py

---

## ğŸ” Evaluation Strategy

The evaluation of LLM responses was done manually based on the following criteria:

- **Accuracy**: Does the LLM's response match the true statistical result?
- **Reasoning**: Does it provide a correct explanation or justification?
- **Format**: Are units, thresholds, and values correctly interpreted?

Each evaluation is recorded in `llm_questions.md` under the **Evaluation** section.

---


## ğŸ“Š LLM Evaluation Summary

Out of **16 total questions**:

- âœ… **14 answers were fully correct**  
- âš ï¸ **1 answer was partially correct** (Q15 â€“ missed a few additional outlier countries)  
- âŒ **1 answer was incorrect** (Q13 â€“ stated a wrong threshold for social support)

Overall, the LLM demonstrated **high accuracy** and **strong reasoning skills**, especially on straightforward statistical queries.


---


## ğŸ“ˆ Skills Practiced

- Python scripting and automation  
- Data analysis with Pandas  
- Correlation and statistical ranking  
- Normalization and scoring logic  
- Evaluating AI/LLM outputs against factual computation  
- Reproducible research and documentation  


---
## âœ… Summary of Activity

I explored descriptive and advanced statistical concepts using the World Happiness Report 2019. I created Python scripts to answer complex questions, evaluated an LLM's output against these factual answers, and reflected on both human and AI-based interpretation of data.
